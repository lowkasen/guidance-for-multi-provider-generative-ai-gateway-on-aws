#This default config file aims to support most popular model providers out of the box

#In general, the model name used by the client will be the same as the ones from the provider (For example, you will use "anthropic.claude-3-5-sonnet-20240620-v1:0" when you're calling LiteLLM just like you would when calling Amazon Bedrock directly)
#In the case where there are model name conflicts, a prefix will be used (For example, the Azure and the openAI model names conflict, so when you are using Azure, you will use "azure/gpt-4o-realtime-preview-2024-10-01")

#Some model providers require additional user-specific configuration (such as Azure which requires you to specify your own api_base with your resource name, and your api_version). 
#In this case, the provider is commented out, and you should uncomment it and provide your specific info

#For more detailed information about each provider, refer to the docs: https://docs.litellm.ai/docs/providers

#If you are not interested in a particular provider, just remove it from your config.yaml, and redeploy, and it will no longer show up in your LiteLLM deployment

#If a particular provider is not working, double check your .env file, and make sure you have provided a valid api key for that provider, and then redeploy

model_list:
  #OpenAI Models
  - model_name: gpt-4o-realtime-preview-2024-10-01
    litellm_params:
      model: openai/gpt-4o-realtime-preview-2024-10-01

  - model_name: o1-mini
    litellm_params:
      model: openai/o1-mini

  - model_name: o1-preview
    litellm_params:
      model: openai/o1-preview

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini

  - model_name: gpt-4o-mini-2024-07-18
    litellm_params:
      model: openai/gpt-4o-mini-2024-07-18

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o

  - model_name: gpt-4o-2024-08-06
    litellm_params:
      model: openai/gpt-4o-2024-08-06

  - model_name: gpt-4o-2024-05-13
    litellm_params:
      model: openai/gpt-4o-2024-05-13

  - model_name: gpt-4-turbo
    litellm_params:
      model: openai/gpt-4-turbo

  - model_name: gpt-4-0125-preview
    litellm_params:
      model: openai/gpt-4-0125-preview

  - model_name: gpt-4-1106-preview
    litellm_params:
      model: openai/gpt-4-1106-preview

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo

  - model_name: gpt-3.5-turbo-1106
    litellm_params:
      model: openai/gpt-3.5-turbo-1106

  - model_name: gpt-3.5-turbo-0301
    litellm_params:
      model: openai/gpt-3.5-turbo-0301

  - model_name: gpt-3.5-turbo-0613
    litellm_params:
      model: openai/gpt-3.5-turbo-0613

  - model_name: gpt-3.5-turbo-16k
    litellm_params:
      model: openai/gpt-3.5-turbo-16k

  - model_name: gpt-3.5-turbo-16k-0613
    litellm_params:
      model: openai/gpt-3.5-turbo-16k-0613

  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4

  - model_name: gpt-4-0314
    litellm_params:
      model: openai/gpt-4-0314

  - model_name: gpt-4-0613
    litellm_params:
      model: openai/gpt-4-0613

  - model_name: gpt-4-32k
    litellm_params:
      model: openai/gpt-4-32k

  - model_name: gpt-4-32k-0314
    litellm_params:
      model: openai/gpt-4-32k-0314

  - model_name: gpt-4-32k-0613
    litellm_params:
      model: openai/gpt-4-32k-0613
  
  - model_name: gpt-4-vision-preview
    litellm_params:
      model: openai/gpt-4-vision-preview

  - model_name: text-embedding-3-small
    litellm_params:
      model: openai/text-embedding-3-small

  - model_name: text-embedding-3-large
    litellm_params:
      model: openai/text-embedding-3-large

  #Gemini Models
  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash

  - model_name: gemini-1.5-flash-8b
    litellm_params:
      model: gemini/gemini-1.5-flash-8b

  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro

  #Anthropic Models
  - model_name: claude-3-5-sonnet-20240620
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20240620
  
  - model_name: claude-3-haiku-20240307
    litellm_params:
      model: anthropic/claude-3-haiku-20240307

  - model_name: claude-3-opus-20240229
    litellm_params:
      model: anthropic/claude-3-opus-20240229

  - model_name: claude-3-sonnet-20240229
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229

  #Mistral Models
  - model_name: mistral-small-latest
    litellm_params:
      model: mistral/mistral-small-latest

  - model_name: mistral-medium-latest
    litellm_params:
      model: mistral/mistral-medium-latest

  - model_name: mistral-large-2407
    litellm_params:
      model: mistral/mistral-large-2407

  - model_name: mistral-large-latest
    litellm_params:
      model: mistral/mistral-large-latest

  - model_name: open-mistral-7b
    litellm_params:
      model: mistral/open-mistral-7b

  - model_name: open-mixtral-8x7b
    litellm_params:
      model: mistral/open-mixtral-8x7b

  - model_name: open-mixtral-8x22b
    litellm_params:
      model: mistral/open-mixtral-8x22b

  - model_name: codestral-latest
    litellm_params:
      model: mistral/codestral-latest

  - model_name: open-mistral-nemo
    litellm_params:
      model: mistral/open-mistral-nemo

  - model_name: open-mistral-nemo-2407
    litellm_params:
      model: mistral/open-mistral-nemo-2407

  - model_name: open-codestral-mamba
    litellm_params:
      model: mistral/open-codestral-mamba

  - model_name: codestral-mamba-latest
    litellm_params:
      model: mistral/codestral-mamba-latest

  #Codestral API Models
  - model_name: text-completion-codestral/codestral-latest
    litellm_params:
      model: text-completion-codestral/codestral-latest

  - model_name: text-completion-codestral/codestral-2405
    litellm_params:
      model: text-completion-codestral/codestral-2405

  #Cohere Models
  - model_name: command-r-plus-08-2024
    litellm_params:
      model: cohere/command-r-plus-08-2024

  - model_name: command-r-08-2024
    litellm_params:
      model: cohere/command-r-08-2024

  - model_name: command-r-plus
    litellm_params:
      model: cohere/command-r-plus

  - model_name: command-r
    litellm_params:
      model: cohere/command-r

  - model_name: command-light
    litellm_params:
      model: cohere/command-light

  - model_name: command-nightly
    litellm_params:
      model: cohere/command-nightly

  #Huggingface Models 
  #(Follow the below pattern to support any Huggingface serverless model you want to use from this list: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation) 
  #(For custom models, text classification models, or dedicated inference endpoints, refer to the docs: https://docs.litellm.ai/docs/providers/huggingface)
  - model_name: meta-llama/Llama-3.2-1B
    litellm_params:
      model: huggingface/meta-llama/Llama-3.2-1B

  - model_name: meta-llama/Llama-3.2-3B-Instruct
    litellm_params:
      model: huggingface/meta-llama/Llama-3.2-3B-Instruct

  - model_name: meta-llama/Llama-3.2-1B-Instruct
    litellm_params:
      model: huggingface/meta-llama/Llama-3.2-1B-Instruct
  
  - model_name: meta-llama/Llama-3.2-3B
    litellm_params:
      model: huggingface/meta-llama/Llama-3.2-3B

  - model_name: meta-llama/Llama-3.1-70B-Instruct
    litellm_params:
      model: huggingface/meta-llama/Llama-3.1-70B-Instruct

  - model_name: mistralai/Mixtral-8x7B-Instruct-v0.1
    litellm_params:
      model: huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1

  - model_name: mistralai/Mistral-7B-v0.1
    litellm_params:
      model: huggingface/mistralai/Mistral-7B-v0.1

  - model_name: mistralai/Mistral-7B-Instruct-v0.3
    litellm_params:
      model: huggingface/mistralai/Mistral-7B-Instruct-v0.3

  - model_name: codellama/CodeLlama-7b-hf
    litellm_params:
      model: huggingface/codellama/CodeLlama-7b-hf

  - model_name: bigcode/starcoder2-15b
    litellm_params:
      model: huggingface/bigcode/starcoder2-15b

  #Nvidia NIM models (Supports all Nvidia nim models following the same pattern as below) 
  #More config examples here: https://docs.litellm.ai/docs/providers/nvidia_nim 
  #Full list of models here: https://docs.api.nvidia.com/nim/reference/models-1
  - model_name: nvidia/nemotron-4-340b-reward
    litellm_params:
      model: nvidia_nim/nvidia/nemotron-4-340b-reward

  - model_name: nvidia/nemotron-4-340b-instruct
    litellm_params:
      model: nvidia_nim/nvidia/nemotron-4-340b-instruct

  - model_name: meta/llama3-70b
    litellm_params:
      model: nvidia_nim/meta/llama3-70b

  #XAI Models
  - model_name: grok-beta
    litellm_params:
      model: xai/grok-beta

  - model_name: grok-vision-beta
    litellm_params:
      model: xai/grok-vision-beta

  #Perplexity AI Models
  - model_name: "pplx-7b-chat"
    litellm_params:
      model: "perplexity/pplx-7b-chat"

  - model_name: "pplx-70b-chat"
    litellm_params:
      model: "perplexity/pplx-70b-chat"

  - model_name: "pplx-7b-online"
    litellm_params:
      model: "perplexity/pplx-7b-online"

  - model_name: "pplx-70b-online"
    litellm_params:
      model: "perplexity/pplx-70b-online"

  #Groq Models
  - model_name: llama-3.1-8b-instant
    litellm_params:
      model: groq/llama-3.1-8b-instant
  
  - model_name: llama-3.1-70b-versatile
    litellm_params:
      model: groq/llama-3.1-70b-versatile
  
  - model_name: llama3-8b-8192
    litellm_params:
      model: groq/llama3-8b-8192
  
  - model_name: llama3-70b-8192
    litellm_params:
      model: groq/llama3-70b-8192
  
  - model_name: mixtral-8x7b-32768
    litellm_params:
      model: groq/mixtral-8x7b-32768

  #Github Models
  - model_name: github/llama-3.1-8b-instant 
    litellm_params:
      model: github/llama-3.1-8b-instant

  - model_name: github/llama-3.1-70b-versatile 
    litellm_params:
      model: github/llama-3.1-70b-versatile

  - model_name: github/llama3-8b-8192 
    litellm_params:
      model: github/llama3-8b-8192

  - model_name: github/llama3-70b-8192 
    litellm_params:
      model: github/llama3-70b-8192

  - model_name: github/mixtral-8x7b-32768 
    litellm_params:
      model: github/mixtral-8x7b-32768

  - model_name: github/gemma-7b-it 
    litellm_params:
      model: github/gemma-7b-it

  #Deepseek Models
  - model_name: deepseek-chat 
    litellm_params:
      model: deepseek/deepseek-chat

  - model_name: deepseek-coder 
    litellm_params:
      model: deepseek/deepseek-coder

  #AI21 Models
  - model_name: jamba-1.5-mini
    litellm_params:
      model: ai21/jamba-1.5-mini
  
  - model_name: jamba-1.5-large
    litellm_params:
      model: ai21/jamba-1.5-large

#Full details on guardrails here: https://docs.litellm.ai/docs/proxy/guardrails/bedrock
# guardrails:
#   - guardrail_name: "bedrock-pre-guard"
#     litellm_params:
#       guardrail: bedrock
#       mode: "during_call" # supported values: "pre_call", "post_call", "during_call"
#       guardrailIdentifier: ff6ujrregl1q # your guardrail ID on bedrock
#       guardrailVersion: "1"         # your guardrail version on bedrock
#       default_on: true # enforces the guardrail serverside for all models. Caller does not need to pass in the name of the guardrail for it to be enforced.

router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
  enable_pre_call_check: true
  # context_window_fallbacks: [{"gpt-4": ["anthropic.claude-3-5-sonnet-20240620-v1:0"]}] #Configure fallbacks for context window exeeded errors (In this example, we will fall back to Claude Sonnet if over 8000 tokens, which is gpt-4's limit)
  # fallbacks: [{"gpt-4o": ["anthropic.claude-3-5-sonnet-20240620-v1:0"]}] #Configure fallbacks for any other error
  # default_fallbacks: ["anthropic.claude-3-haiku-20240307-v1:0"] #Configure fallbacks for any error for every model (the above fallback configurations override this one)

environment_variables:
  STORE_MODEL_IN_DB: 'True'
  LITELLM_LOG: "DEBUG"

litellm_settings:
  cache: True
  debug: True
  detailed_debug: True
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST # Redis server hostname or IP address
    port: os.environ/REDIS_PORT # Redis server port (as a string)
    password: os.environ/REDIS_PASSWORD # Redis server password
    
  max_budget: 1000000000.0 # (float) sets max budget in dollars across the entire proxy across all API keys. Note, the budget does not apply to the master key. That is the only exception.
  budget_duration: 1mo # (str) frequency of budget reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").

  max_internal_user_budget: 1000000000.0 # (float) sets default budget in dollars for each internal user. (Doesn't apply to Admins. Doesn't apply to Teams. Doesn't apply to master key)
  internal_user_budget_duration: "1mo" # (str) frequency of budget reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").

  success_callback: ["s3"]
  failure_callback: ["s3"]
  service_callback: ["s3"]
  s3_callback_params:
    s3_bucket_name:
    s3_region_name:
  #type: redis-semantic
  #similarity_threshold: 0.8   # similarity threshold for semantic cache
  #redis_semantic_cache_embedding_model: text-embedding-ada-002 # only works with text-embedding-ada-002 for now... https://github.com/BerriAI/litellm/issues/4001

  #ttl: Optional[float]
  #default_in_memory_ttl: Optional[float]
  #default_in_redis_ttl: Optional[float]