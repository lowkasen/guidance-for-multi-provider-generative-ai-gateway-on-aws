#This default config file aims to support most popular model providers out of the box

#In general, the model name used by the client will be the same as the ones from the provider (For example, you will use "anthropic.claude-3-5-sonnet-20240620-v1:0" when you're calling LiteLLM just like you would when calling Amazon Bedrock directly)
#In the case where there are model name conflicts, a prefix will be used (For example, the Azure and the openAI model names conflict, so when you are using Azure, you will use "azure/gpt-4o-realtime-preview-2024-10-01")

#Some model providers require additional user-specific configuration (such as Azure which requires you to specify your own api_base with your resource name, and your api_version).
#In this case, the provider is commented out, and you should uncomment it and provide your specific info

#For more detailed information about each provider, refer to the docs: https://docs.litellm.ai/docs/providers

#If you are not interested in a particular provider, just remove it from your config.yaml, and redeploy, and it will no longer show up in your LiteLLM deployment

#If a particular provider is not working, double check your .env file, and make sure you have provided a valid api key for that provider, and then redeploy

#Full details on guardrails here: https://docs.litellm.ai/docs/proxy/guardrails/bedrock
# guardrails:
#   - guardrail_name: "bedrock-pre-guard"
#     litellm_params:
#       guardrail: bedrock
#       mode: "during_call" # supported values: "pre_call", "post_call", "during_call"
#       guardrailIdentifier: ff6ujrregl1q # your guardrail ID on bedrock
#       guardrailVersion: "1"         # your guardrail version on bedrock
#       default_on: true # enforces the guardrail serverside for all models. Caller does not need to pass in the name of the guardrail for it to be enforced.
router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
  enable_pre_call_check: true
  # context_window_fallbacks: [{"gpt-4": ["anthropic.claude-3-5-sonnet-20240620-v1:0"]}] #Configure fallbacks for context window exeeded errors (In this example, we will fall back to Claude Sonnet if over 8000 tokens, which is gpt-4's limit)
  # fallbacks: [{"gpt-4o": ["anthropic.claude-3-5-sonnet-20240620-v1:0"]}] #Configure fallbacks for any other error
  # default_fallbacks: ["anthropic.claude-3-haiku-20240307-v1:0"] #Configure fallbacks for any error for every model (the above fallback configurations override this one)
environment_variables:
  STORE_MODEL_IN_DB: "True"
  LITELLM_LOG: "DEBUG"
general_settings:
  store_model_in_db: true
  store_prompts_in_spend_logs: true
litellm_settings:
  cache: True
  debug: True
  detailed_debug: True
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST # Redis server hostname or IP address
    port: os.environ/REDIS_PORT # Redis server port (as a string)
    password: os.environ/REDIS_PASSWORD # Redis server password
  max_budget: 1000000000.0 # (float) sets max budget in dollars across the entire proxy across all API keys. Note, the budget does not apply to the master key. That is the only exception.
  budget_duration: 1mo # (str) frequency of budget reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").
  max_internal_user_budget: 1000000000.0 # (float) sets default budget in dollars for each internal user. (Doesn't apply to Admins. Doesn't apply to Teams. Doesn't apply to master key)
  internal_user_budget_duration: "1mo" # (str) frequency of budget reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d"), months ("1mo").
  success_callback: ["s3", "langfuse"]
  failure_callback: ["s3"]
  service_callback: ["s3"]
  s3_callback_params:
    s3_bucket_name: litellm-logs-20250806033124646400000001
    s3_region_name: ap-southeast-1
    #type: redis-semantic
    #similarity_threshold: 0.8   # similarity threshold for semantic cache
    #redis_semantic_cache_embedding_model: text-embedding-ada-002 # only works with text-embedding-ada-002 for now... https://github.com/BerriAI/litellm/issues/4001
#ttl: Optional[float]
#default_in_memory_ttl: Optional[float]
#default_in_redis_ttl: Optional[float]

model_list:
  # #OpenAI Models
  # - model_name: gpt-4o-realtime-preview-2024-10-01
  #   litellm_params:
  #     model: openai/gpt-4o-realtime-preview-2024-10-01
  # - model_name: o1-mini
  #   litellm_params:
  #     model: openai/o1-mini
  # - model_name: o1-preview
  #   litellm_params:
  #     model: openai/o1-preview
  # - model_name: gpt-4o-mini
  #   litellm_params:
  #     model: openai/gpt-4o-mini
  # - model_name: gpt-4o-mini-2024-07-18
  #   litellm_params:
  #     model: openai/gpt-4o-mini-2024-07-18
  # - model_name: gpt-4o
  #   litellm_params:
  #     model: openai/gpt-4o
  # - model_name: gpt-4o-2024-08-06
  #   litellm_params:
  #     model: openai/gpt-4o-2024-08-06
  # - model_name: gpt-4o-2024-05-13
  #   litellm_params:
  #     model: openai/gpt-4o-2024-05-13
  # - model_name: gpt-4-turbo
  #   litellm_params:
  #     model: openai/gpt-4-turbo
  # - model_name: gpt-4-0125-preview
  #   litellm_params:
  #     model: openai/gpt-4-0125-preview
  # - model_name: gpt-4-1106-preview
  #   litellm_params:
  #     model: openai/gpt-4-1106-preview
  # - model_name: gpt-3.5-turbo
  #   litellm_params:
  #     model: openai/gpt-3.5-turbo
  # - model_name: gpt-3.5-turbo-1106
  #   litellm_params:
  #     model: openai/gpt-3.5-turbo-1106
  # - model_name: gpt-3.5-turbo-0301
  #   litellm_params:
  #     model: openai/gpt-3.5-turbo-0301
  # - model_name: gpt-3.5-turbo-0613
  #   litellm_params:
  #     model: openai/gpt-3.5-turbo-0613
  # - model_name: gpt-3.5-turbo-16k
  #   litellm_params:
  #     model: openai/gpt-3.5-turbo-16k
  # - model_name: gpt-3.5-turbo-16k-0613
  #   litellm_params:
  #     model: openai/gpt-3.5-turbo-16k-0613
  # - model_name: gpt-4
  #   litellm_params:
  #     model: openai/gpt-4
  # - model_name: gpt-4-0314
  #   litellm_params:
  #     model: openai/gpt-4-0314
  # - model_name: gpt-4-0613
  #   litellm_params:
  #     model: openai/gpt-4-0613
  # - model_name: gpt-4-32k
  #   litellm_params:
  #     model: openai/gpt-4-32k
  # - model_name: gpt-4-32k-0314
  #   litellm_params:
  #     model: openai/gpt-4-32k-0314
  # - model_name: gpt-4-32k-0613
  #   litellm_params:
  #     model: openai/gpt-4-32k-0613
  # - model_name: gpt-4-vision-preview
  #   litellm_params:
  #     model: openai/gpt-4-vision-preview
  # - model_name: text-embedding-3-small
  #   litellm_params:
  #     model: openai/text-embedding-3-small
  # - model_name: text-embedding-3-large
  #   litellm_params:
  #     model: openai/text-embedding-3-large
  # #Gemini Models
  # - model_name: gemini-1.5-flash
  #   litellm_params:
  #     model: gemini/gemini-1.5-flash
  # - model_name: gemini-1.5-flash-8b
  #   litellm_params:
  #     model: gemini/gemini-1.5-flash-8b
  # - model_name: gemini-1.5-pro
  #   litellm_params:
  #     model: gemini/gemini-1.5-pro
  # #Anthropic Models
  # - model_name: claude-3-5-sonnet-20240620
  #   litellm_params:
  #     model: anthropic/claude-3-5-sonnet-20240620
  # - model_name: claude-3-haiku-20240307
  #   litellm_params:
  #     model: anthropic/claude-3-haiku-20240307
  # - model_name: claude-3-opus-20240229
  #   litellm_params:
  #     model: anthropic/claude-3-opus-20240229
  # - model_name: claude-3-sonnet-20240229
  #   litellm_params:
  #     model: anthropic/claude-3-sonnet-20240229
  # #Mistral Models
  # - model_name: mistral-small-latest
  #   litellm_params:
  #     model: mistral/mistral-small-latest
  # - model_name: mistral-medium-latest
  #   litellm_params:
  #     model: mistral/mistral-medium-latest
  # - model_name: mistral-large-2407
  #   litellm_params:
  #     model: mistral/mistral-large-2407
  # - model_name: mistral-large-latest
  #   litellm_params:
  #     model: mistral/mistral-large-latest
  # - model_name: open-mistral-7b
  #   litellm_params:
  #     model: mistral/open-mistral-7b
  # - model_name: open-mixtral-8x7b
  #   litellm_params:
  #     model: mistral/open-mixtral-8x7b
  # - model_name: open-mixtral-8x22b
  #   litellm_params:
  #     model: mistral/open-mixtral-8x22b
  # - model_name: codestral-latest
  #   litellm_params:
  #     model: mistral/codestral-latest
  # - model_name: open-mistral-nemo
  #   litellm_params:
  #     model: mistral/open-mistral-nemo
  # - model_name: open-mistral-nemo-2407
  #   litellm_params:
  #     model: mistral/open-mistral-nemo-2407
  # - model_name: open-codestral-mamba
  #   litellm_params:
  #     model: mistral/open-codestral-mamba
  # - model_name: codestral-mamba-latest
  #   litellm_params:
  #     model: mistral/codestral-mamba-latest
  # #Codestral API Models
  # - model_name: text-completion-codestral/codestral-latest
  #   litellm_params:
  #     model: text-completion-codestral/codestral-latest
  # - model_name: text-completion-codestral/codestral-2405
  #   litellm_params:
  #     model: text-completion-codestral/codestral-2405
  # #Cohere Models
  # - model_name: command-r-plus-08-2024
  #   litellm_params:
  #     model: cohere/command-r-plus-08-2024
  # - model_name: command-r-08-2024
  #   litellm_params:
  #     model: cohere/command-r-08-2024
  # - model_name: command-r-plus
  #   litellm_params:
  #     model: cohere/command-r-plus
  # - model_name: command-r
  #   litellm_params:
  #     model: cohere/command-r
  # - model_name: command-light
  #   litellm_params:
  #     model: cohere/command-light
  # - model_name: command-nightly
  #   litellm_params:
  #     model: cohere/command-nightly
  # #Huggingface Models
  # #(Follow the below pattern to support any Huggingface serverless model you want to use from this list: https://huggingface.co/models?inference=warm&pipeline_tag=text-generation)
  # #(For custom models, text classification models, or dedicated inference endpoints, refer to the docs: https://docs.litellm.ai/docs/providers/huggingface)
  # - model_name: meta-llama/Llama-3.2-1B
  #   litellm_params:
  #     model: huggingface/meta-llama/Llama-3.2-1B
  # - model_name: meta-llama/Llama-3.2-3B-Instruct
  #   litellm_params:
  #     model: huggingface/meta-llama/Llama-3.2-3B-Instruct
  # - model_name: meta-llama/Llama-3.2-1B-Instruct
  #   litellm_params:
  #     model: huggingface/meta-llama/Llama-3.2-1B-Instruct
  # - model_name: meta-llama/Llama-3.2-3B
  #   litellm_params:
  #     model: huggingface/meta-llama/Llama-3.2-3B
  # - model_name: meta-llama/Llama-3.1-70B-Instruct
  #   litellm_params:
  #     model: huggingface/meta-llama/Llama-3.1-70B-Instruct
  # - model_name: mistralai/Mixtral-8x7B-Instruct-v0.1
  #   litellm_params:
  #     model: huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1
  # - model_name: mistralai/Mistral-7B-v0.1
  #   litellm_params:
  #     model: huggingface/mistralai/Mistral-7B-v0.1
  # - model_name: mistralai/Mistral-7B-Instruct-v0.3
  #   litellm_params:
  #     model: huggingface/mistralai/Mistral-7B-Instruct-v0.3
  # - model_name: codellama/CodeLlama-7b-hf
  #   litellm_params:
  #     model: huggingface/codellama/CodeLlama-7b-hf
  # - model_name: bigcode/starcoder2-15b
  #   litellm_params:
  #     model: huggingface/bigcode/starcoder2-15b
  # #Nvidia NIM models (Supports all Nvidia nim models following the same pattern as below)
  # #More config examples here: https://docs.litellm.ai/docs/providers/nvidia_nim
  # #Full list of models here: https://docs.api.nvidia.com/nim/reference/models-1
  # - model_name: nvidia/nemotron-4-340b-reward
  #   litellm_params:
  #     model: nvidia_nim/nvidia/nemotron-4-340b-reward
  # - model_name: nvidia/nemotron-4-340b-instruct
  #   litellm_params:
  #     model: nvidia_nim/nvidia/nemotron-4-340b-instruct
  # - model_name: meta/llama3-70b
  #   litellm_params:
  #     model: nvidia_nim/meta/llama3-70b
  # #XAI Models
  # - model_name: grok-beta
  #   litellm_params:
  #     model: xai/grok-beta
  # - model_name: grok-vision-beta
  #   litellm_params:
  #     model: xai/grok-vision-beta
  # #Perplexity AI Models
  # - model_name: "pplx-7b-chat"
  #   litellm_params:
  #     model: "perplexity/pplx-7b-chat"
  # - model_name: "pplx-70b-chat"
  #   litellm_params:
  #     model: "perplexity/pplx-70b-chat"
  # - model_name: "pplx-7b-online"
  #   litellm_params:
  #     model: "perplexity/pplx-7b-online"
  # - model_name: "pplx-70b-online"
  #   litellm_params:
  #     model: "perplexity/pplx-70b-online"
  # #Groq Models
  # - model_name: llama-3.1-8b-instant
  #   litellm_params:
  #     model: groq/llama-3.1-8b-instant
  # - model_name: llama-3.1-70b-versatile
  #   litellm_params:
  #     model: groq/llama-3.1-70b-versatile
  # - model_name: llama3-8b-8192
  #   litellm_params:
  #     model: groq/llama3-8b-8192
  # - model_name: llama3-70b-8192
  #   litellm_params:
  #     model: groq/llama3-70b-8192
  # - model_name: mixtral-8x7b-32768
  #   litellm_params:
  #     model: groq/mixtral-8x7b-32768
  # #Github Models
  # - model_name: github/llama-3.1-8b-instant
  #   litellm_params:
  #     model: github/llama-3.1-8b-instant
  # - model_name: github/llama-3.1-70b-versatile
  #   litellm_params:
  #     model: github/llama-3.1-70b-versatile
  # - model_name: github/llama3-8b-8192
  #   litellm_params:
  #     model: github/llama3-8b-8192
  # - model_name: github/llama3-70b-8192
  #   litellm_params:
  #     model: github/llama3-70b-8192
  # - model_name: github/mixtral-8x7b-32768
  #   litellm_params:
  #     model: github/mixtral-8x7b-32768
  # - model_name: github/gemma-7b-it
  #   litellm_params:
  #     model: github/gemma-7b-it
  # #Deepseek Models
  # - model_name: deepseek-chat
  #   litellm_params:
  #     model: deepseek/deepseek-chat
  # - model_name: deepseek-coder
  #   litellm_params:
  #     model: deepseek/deepseek-coder
  # #AI21 Models
  # - model_name: jamba-1.5-mini
  #   litellm_params:
  #     model: ai21/jamba-1.5-mini
  # - model_name: jamba-1.5-large
  #   litellm_params:
  #     model: ai21/jamba-1.5-large
  #Bedrock Models
  - model_name: anthropic.claude-3-haiku-20240307-v1:0
    litellm_params:
      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
  - model_name: anthropic.claude-3-5-sonnet-20240620-v1:0
    litellm_params:
      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0
  # - model_name: cohere.embed-english-v3
  #   litellm_params:
  #     model: bedrock/cohere.embed-english-v3
  #     drop_params: true #Needed to avoid errors when encoding_format is passed in by openai python client
  # - model_name: cohere.embed-multilingual-v3
  #   litellm_params:
  #     model: bedrock/cohere.embed-multilingual-v3
  - model_name: apac.anthropic.claude-3-5-sonnet-20240620-v1:0
    litellm_params:
      model: bedrock/apac.anthropic.claude-3-5-sonnet-20240620-v1:0
  - model_name: apac.anthropic.claude-3-sonnet-20240229-v1:0
    litellm_params:
      model: bedrock/apac.anthropic.claude-3-sonnet-20240229-v1:0
  - model_name: apac.anthropic.claude-3-haiku-20240307-v1:0
    litellm_params:
      model: bedrock/apac.anthropic.claude-3-haiku-20240307-v1:0
  - model_name: apac.anthropic.claude-3-5-sonnet-20241022-v2:0
    litellm_params:
      model: bedrock/apac.anthropic.claude-3-5-sonnet-20241022-v2:0
  - model_name: apac.amazon.nova-lite-v1:0
    litellm_params:
      model: bedrock/apac.amazon.nova-lite-v1:0
  - model_name: apac.amazon.nova-micro-v1:0
    litellm_params:
      model: bedrock/apac.amazon.nova-micro-v1:0
  - model_name: apac.amazon.nova-pro-v1:0
    litellm_params:
      model: bedrock/apac.amazon.nova-pro-v1:0
